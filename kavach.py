# -*- coding: utf-8 -*-
"""Kavach.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q0qFkoVp_HyzkThKZj6hLabFBW5XWpFu

Keyword Extraction
"""

!pip install spacy

import spacy
from spacy.lang.en.stop_words import STOP_WORDS

nlp = spacy.load('en_core_web_sm')

input_string = input("Enter a string of around 25 words: ")
doc = nlp(input_string)

filtered_tokens = [token.text for token in doc if not token.is_stop and not token.is_punct]

noun_chunks = list(doc.noun_chunks)
keywords = []
for chunk in noun_chunks:
    if chunk.root.text in filtered_tokens:
        keywords.append(chunk.text)
        if len(keywords) == 5:
            break

output_sentence = " ".join(keywords)
print("Keywords: ", keywords)
print("Output sentence: ", output_sentence)

"""Fake News Detection"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

main_sentence = "Elon Musk is worried about Ai development"

sentences = [
    "Elon Musk and others urge AI pause, citing risks to society",
    "Elon Musk among experts urging a halt to AI training",
    "Elon Musk loves A.I",
    "Elon Musk's AI History May Be Behind His Call To Pause Development",
    "Elon Musk, who co-founded firm behind ChatGPT, warns A.I. is 'one of the biggest risks' to civilization",
    "Elon Musk and experts say AI development should be paused immediately MakeUseOf",
    "Artificial intelligence stresses me out, says Elon Musk",
    "Elon Musk joins hundreds calling for a six-month pause on AI development in an open letter",
    "Why Elon Musk fears artificial intelligence",
    "Musk, scientists call for halt to AI race sparked by ChatGPT"
]

vectorizer = TfidfVectorizer(stop_words='english')
X_main = vectorizer.fit_transform([main_sentence] + sentences)
X_sentences = vectorizer.transform(sentences)
X = np.vstack([X_main.toarray(), X_sentences.toarray()])

clf = MultinomialNB()
clf.fit(X[:1], [1])
pred_labels = clf.predict(X[1:])

for i, sentence in enumerate(sentences):
    print(f"Sentence {i+1}: {sentence}")
    if pred_labels[i] == 0:
        print("Accuracy: 0.0%")
        print("Fake news")
    else:
        print(f"Accuracy: {clf.predict_proba(X[1:])[i][0]*100:.2f}%")
        print("Correct news")
    print()

"""Using pre trained BERT"""

!pip install transformers

from sklearn.linear_model import LogisticRegression
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

!pip install torch

import torch
sentences_embeddings = []
for sentence in [main_sentence] + sentences:
    encoded_dict = tokenizer.encode_plus(
        sentence,
        add_special_tokens=True,
        max_length=64,
        padding='max_length',
        return_attention_mask=True,
        return_tensors='pt',
    )
    with torch.no_grad():
        outputs = model(encoded_dict['input_ids'], encoded_dict['attention_mask'])
        last_hidden_states = outputs[0]
        sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze().numpy()
        sentences_embeddings.append(sentence_embedding)

cosine_similarities = []
for i in range(1, len(sentences_embeddings)):
    cosine_sim = np.dot(sentences_embeddings[0], sentences_embeddings[i]) / (np.linalg.norm(sentences_embeddings[0]) * np.linalg.norm(sentences_embeddings[i]))
    cosine_similarities.append(cosine_sim)

import pandas as pd
df = pd.DataFrame({'sentence': sentences, 'cosine_similarity': cosine_similarities})
tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)
tfidf_features = tfidf_vectorizer.fit_transform(df['sentence'])

X = np.hstack((tfidf_features.toarray(), df['cosine_similarity'].values.reshape(-1, 1)))
y = np.array([0] * (len(sentences)-1) + [1])
clf = LogisticRegression(random_state=42, max_iter=1000).fit(X, y)

# predict the labels for each sentence and print the results
for i, sentence in enumerate(sentences):
    X_test = np.hstack((tfidf_vectorizer.transform([sentence]).toarray(), np.array([cosine_similarities[i-1]]).reshape(-1, 1)))
    y_pred = clf.predict(X_test)[0]
    if y_pred == 1:
        accuracy = clf.predict_proba(X_test)[0][1]*100
        print(f"Sentence: {sentence}")
        print(f"Accuracy: {accuracy:.2f}%")
        if accuracy >= 90:
            print("Correct news")
        else:
            print("Fake news")
    else:
        accuracy = clf.predict_proba(X_test)[0][0]*100
        print(f"Sentence: {sentence}")
        print(f"Accuracy: {accuracy:.2f}%")
        if accuracy >= 90:
            print("Correct news")
        else:
            print("Fake news")
    print()

import matplotlib.pyplot as plt
accuracies = []
for i, sentence in enumerate(sentences):
    X_test = np.hstack((tfidf_vectorizer.transform([sentence]).toarray(), np.array([cosine_similarities[i-1]]).reshape(-1, 1)))
    y_pred = clf.predict(X_test)[0]
    if y_pred == 1:
        accuracy = clf.predict_proba(X_test)[0][1]*100
        accuracies.append(accuracy)
    else:
        accuracy = clf.predict_proba(X_test)[0][0]*100
        accuracies.append(accuracy)

plt.plot(accuracies)
plt.xlabel("Sentence index")
plt.ylabel("Accuracy (%)")
plt.title("Accuracy of sentence classifications")
plt.show()

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, export_graphviz
import graphviz
iris = load_iris()
X = iris.data
y = iris.target
clf = DecisionTreeClassifier()
clf.fit(X, y)
dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True)
graph = graphviz.Source(dot_data)
graph.render("iris_decision_tree")